{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Project 1<br>Binary Classification with Logistic Regression<br>Due: April 23, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student ID1:**\n",
    "* **Student ID2:**\n",
    "* **Student ID3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this project, you are going to implement logistic regression from scratch. You are provided\n",
    "a subset of the famous handwritten digit dataset called MNIST. In the subset, you will find images of digit 1 and 5. Therefore, you will be solving a binary classification problem. The project includes feature extraction, model training, and evaluation steps.\n",
    "\n",
    "First, you will load and visualize the data portion we have provided to you and then extract two different set of features to build a classifier on. When you extracted the desired features, you will run your logistic regression implementation with gradient descent on the representations to classify digits into 1 and 5. You will experiment with different learning rates  and regularization parameter ($\\lambda$) and find the optimal $\\lambda$ with 5-fold cross validation. Finally, you will evaluate the implemented models, decide which is the best performing one and visualize a decision boundary.\n",
    "\n",
    "Follow the steps on this notebook that would guide you through the solution step-by-step. Make sure that the notebook reports your work properly and add comments and opinions when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** You are allowed to use third-party libraries such as `numpy` and `matplotlib` to help you implement necessary procedures. However, you should not import any function that accomplishes the task itself. For instance, you can use `numpy` arrays for matrix operations, but you cannot use `scikit-learn` to implement cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Feature Extraction (35 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training/test data and labels as numpy arrays (Hint:`np.load`). Train and test data are 1561x256 and 424x256 dimensional matrices, respectively. Each row in the\n",
    "aforementioned matrices corresponds to an image of a digit. The 256 pixels correspond to a 16x16 image. Label 1 is assigned to digit 1 and label -1 is assigned to digit 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRAIN_SET = np.load('data/train_data.npy')\n",
    "TRAIN_LABELS = np.load('data/train_labels.npy').reshape(-1,1)\n",
    "\n",
    "TEST_SET = np.load('data/test_data.npy')\n",
    "TEST_LABELS = np.load('data/test_labels.npy').reshape(-1,1)\n",
    "\n",
    "TRAIN_ROW, TRAIN_COLUMN = TRAIN_SET.shape\n",
    "TEST_ROW, TEST_COLUMN = TEST_SET.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Display two of the digit images, one for digit 1 and one for digit 5. You can use `imshow` function of `matplotlib` library with a suitable colormap. You will first need to reshape 256 pixels to a 16x16 matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_one_img = TRAIN_SET[np.where(TRAIN_LABELS==1)[0][0]].reshape(16,16)\n",
    "digit_five_img = TRAIN_SET[np.where(TRAIN_LABELS==-1)[0][0]].reshape(16,16)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "fig.add_subplot(121)\n",
    "plt.imshow(digit_one_img, cmap='Greys',  interpolation='nearest')\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.imshow(digit_five_img, cmap='Greys',  interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points) Implementing Representation 1:** Extract the **symmetry** and **average intensity** features discussed in the class (see logistic regression lecture notes). To compute the intensity features, compute the average pixel value of the image, and for the symmetry feature, compute the negative of the norm of the difference between the image and its y-axis symmetrical. Search numpy's documentation to find the suitable function at each step. You should extract these two features for each image in the training and test sets. As a result, you should obtain a training data matrix of size 1561x2 and test data matrix of size 424x2.\n",
    "\n",
    "Throughout the notebook, we will refer the representation with these two features as **Representation 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_TRAIN = np.zeros([TRAIN_ROW, 2])\n",
    "#average intensity\n",
    "R1_TRAIN[:,0] = np.mean(TRAIN_SET, axis=1)\n",
    "#symmetry\n",
    "flipped_set = np.fliplr(TRAIN_SET.reshape(-1,16)).reshape(-1,256)\n",
    "R1_TRAIN[:,1] = -np.mean(np.absolute(TRAIN_SET - flipped_set), axis=1)\n",
    "\n",
    "R1_TEST = np.zeros([TEST_ROW, 2])\n",
    "#average intensity\n",
    "R1_TEST[:,0] = np.mean(TEST_SET, axis=1)\n",
    "#symmetry\n",
    "flipped_set = np.fliplr(TEST_SET.reshape(-1,16)).reshape(-1,256)\n",
    "R1_TEST[:,1] = -np.mean(np.absolute(TEST_SET - flipped_set), axis=1)\n",
    "\n",
    "R1_TRAIN = np.append(np.ones((TRAIN_ROW,1)), R1_TRAIN, axis =1 )\n",
    "R1_TEST = np.append(np.ones((TEST_ROW,1)), R1_TEST, axis =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Provide two scatter plots, one for training and one for test data. The plots should contain the average intensity values in the x-axis and symmetry values in the\n",
    "y-axis. Denote the data points of label 1 with blue marker shaped <font color='blue'>o</font> and the data points of label -1 with a red marker shaped <font color='red'>x</font>. (Hint: check out `plt.scatter` and its `marker` and `color` parameters). Explicitly state the axis labels and figure title for both plots (Hint: `plt.xlabel`, `plt.ylabel`, `plt.title`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "train_plot = fig.add_subplot(121)\n",
    "TRAIN_ONE = R1_TRAIN[np.where(TRAIN_LABELS.reshape(-1,) == 1)]\n",
    "TRAIN_FIVE = R1_TRAIN[np.where(TRAIN_LABELS.reshape(-1,) == -1)]\n",
    "plt.scatter(TRAIN_ONE[:,1], TRAIN_ONE[:,2], facecolors='none', c='blue',marker='o')\n",
    "plt.scatter(TRAIN_FIVE[:,1], TRAIN_FIVE[:,2], facecolors='none', c='red',marker='x')\n",
    "\n",
    "test_plot = fig.add_subplot(122)\n",
    "TEST_ONE = R1_TEST[np.where(TEST_LABELS.reshape(-1,) == 1)]\n",
    "TEST_FIVE = R1_TEST[np.where(TEST_LABELS.reshape(-1,) == -1)]\n",
    "plt.scatter(TEST_ONE[:,1], TEST_ONE[:,2], facecolors='none', c='blue',marker='o')\n",
    "plt.scatter(TEST_FIVE[:,1], TEST_FIVE[:,2], facecolors='none', c='red',marker='x')\n",
    "    \n",
    "train_plot.title.set_text('Train Data')\n",
    "train_plot.set_xlabel('Intensity')\n",
    "train_plot.set_ylabel('Symmetry')\n",
    "\n",
    "\n",
    "test_plot.title.set_text('Test Data')\n",
    "test_plot.set_xlabel('Intensity')\n",
    "test_plot.set_ylabel('Symmetry')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points) Implementing Representation 2:** Come up with an alternative feature extraction approach. The features can again be 2-D, or higher dimensional. If you use any external resource, please cite the references. Explain the feature extraction procedure clearly in your report; if it is an algorithm, provide the algorithm; if it is a function, provide the mathematical expressions. \n",
    "\n",
    "If your proposed features are 2-D or 3-D, provide the scatter plots similar to the previous step.\n",
    "\n",
    "We will refer this representation proposed by you as **Representation 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_stats_from_image(im):\n",
    "    binarized = 1.0 * (im > 0.25)\n",
    "    axis_means = []\n",
    "    for row in binarized:\n",
    "        ones = np.where(row==1)\n",
    "        if len(ones[0]) > 0:\n",
    "            axis_means.append(np.mean(ones[0]))\n",
    "    return np.std(axis_means), np.mean(axis_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPRESENTATION_2_TRAIN = np.empty([TRAIN_SET.shape[0], 2])\n",
    "for idx,im in enumerate(TRAIN_SET):\n",
    "    stats = pixel_stats_from_image(im.reshape(16,16))\n",
    "    REPRESENTATION_2_TRAIN[idx][0] = stats[0]\n",
    "    REPRESENTATION_2_TRAIN[idx][1] = stats[1]\n",
    "    \n",
    "REPRESENTATION_2_TEST = np.empty([TEST_SET.shape[0], 2])\n",
    "for idx,im in enumerate(TEST_SET):\n",
    "    stats = pixel_stats_from_image(im.reshape(16,16))\n",
    "    REPRESENTATION_2_TEST[idx][0] = stats[0]\n",
    "    REPRESENTATION_2_TEST[idx][1] = stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "train_plot = fig.add_subplot(121)\n",
    "colors=['blue' if i==1 else 'red' for i in TRAIN_LABELS]\n",
    "markers=['o' if i==1 else 'x' for i in TRAIN_LABELS]\n",
    "for x, y, c, m in zip(REPRESENTATION_2_TRAIN[:,0], REPRESENTATION_2_TRAIN[:,1], colors, markers):\n",
    "    plt.scatter(x, y, alpha=0.8, facecolors='none', c=c,marker=m)\n",
    "    \n",
    "test_plot = fig.add_subplot(122)\n",
    "colors=['blue' if i==1 else 'red' for i in TEST_LABELS]\n",
    "markers=['o' if i==1 else 'x' for i in TEST_LABELS]\n",
    "for x, y, c, m in zip(REPRESENTATION_2_TEST[:,0], REPRESENTATION_2_TEST[:,1], colors, markers):\n",
    "    plt.scatter(x, y, alpha=0.8, facecolors='none', c=c,marker=m)\n",
    "    \n",
    "train_plot.title.set_text('Train Data')\n",
    "test_plot.title.set_text('Test Data')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Logistic Regression (40 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(20 points)** Implement the logistic regression classifier from scratch with gradient descent and train it using Representation 1 and Representation 2 as inputs. Concatenate\n",
    "1 to your features for the intercept term, such that one data point will look like for 2-D features [1,$x_1$,$x_2$], and the model vector will be [$w_0, w_1, w_2$], where $w_0$ is the intercept parameter. \n",
    "You can refer to lecture notes (Logistic regression slides 29-30) to review the gradient descent learning algorithm and the logistic loss. To implement the gradient of the logistic loss with respect to $w$, first derive its expression by hand. Please include your derivation in your report.\n",
    "\n",
    "To prove that your implementation is converging, keep the loss values at each gradient descent iteration in a numpy array. After the training is finalized, plot the loss values\n",
    "with respect to iteration count (Hint: `plt.plot`). You should observe a decreasing loss as the number of iterations increases. Also, experiment with 5 different learning rates between 0 and 1, and plot the convergence curves for each learning rate in the same plot to observe the effect of the learning rate (step size) on the convergence. \n",
    "\n",
    "To decide when to terminate the gradient descent iterations, check the absolute difference between the current loss value and the loss value of the previous step. If the difference is less than a small number, such as $10^{-5}$, you can exit the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(data):\n",
    "    return 1/(1+np.exp(-data))\n",
    "\n",
    "def predict(weights, data):\n",
    "    probs = sigmoid(np.sum(data@(weights.reshape(-1,1)),axis=1))\n",
    "    probs[probs > 0.5] = 1\n",
    "    probs[probs <= 0.5] = -1\n",
    "    return probs\n",
    "\n",
    "def accuracy(weights, data, label):\n",
    "    pred = predict(weights, data)\n",
    "    return (pred.reshape(-1,1) == label).sum() / label.shape[0]\n",
    "    \n",
    "def calculate_loss(data, label, coefficients):\n",
    "    YWX = np.exp(-label * (data@coefficients))\n",
    "    return np.sum(np.log(1 + YWX),axis=0) / TRAIN_ROW\n",
    "\n",
    "def calculate_gradient(data, label, coefficients):\n",
    "    YX = label * data\n",
    "    YWX = np.exp(label * (data@coefficients))\n",
    "    return np.sum(-YX / (1 + YWX),axis=0) / TRAIN_ROW\n",
    "\n",
    "def gradient_descent(data, label, coefficients, learning_rate):\n",
    "    loss_history = np.zeros((0, 1))\n",
    "    while True:\n",
    "        gradient = calculate_gradient(data, label, coefficients)\n",
    "        coefficients = coefficients - (gradient.reshape(3,1) * learning_rate)\n",
    "        loss_history = np.append(loss_history, calculate_loss(data, label, coefficients))\n",
    "        if (len(loss_history) > 1 and np.abs(loss_history[-1] - loss_history[-2]) < 1e-5):\n",
    "            break\n",
    "    return loss_history, coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRates = np.array([0.01, 0.05, 0.1, 0.2, 0.5])\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for pos in range(len(learningRates)):\n",
    "    coefficients = np.random.normal(0,0.5,3).reshape(3,1)\n",
    "    cost_history, params_optimal = gradient_descent(R1_TRAIN, TRAIN_LABELS, coefficients, learningRates[pos])\n",
    "    plt.plot(range(len(cost_history)), cost_history, colors[pos])\n",
    "\n",
    "plt.title(\"Convergence Graph of Loss Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(params_optimal, R1_TEST, TEST_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Implement logistic regression with $\\ell_2$ norm regularization, $||\\mathbf{w}||_{2}^{2}$ . Show that your implementation is working by visualizing the loss over the iterations again. Visualization for a single learning rate and $\\lambda$ suffices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_calculate_loss(data, label, coefficients, penalty):\n",
    "    return calculate_loss(data, label, coefficients) + ((penalty/2)*(np.linalg.norm(coefficients,2)**2))\n",
    "\n",
    "def regularized_calculate_gradient(data,label,coefficients,penalty):\n",
    "    return calculate_gradient(data, label, coefficients) + (penalty*(np.sum(coefficients)))\n",
    "\n",
    "def regularized_gradient_descent(data, label,coefficients, learning_rate,penalty):\n",
    "    loss_history = np.zeros((0, 1))\n",
    "    while True:\n",
    "        gradient = regularized_calculate_gradient(data, label, coefficients, penalty)\n",
    "        coefficients = coefficients - (gradient.reshape(3,1) * learning_rate)\n",
    "        loss_history = np.append(loss_history, regularized_calculate_loss(data, label, coefficients, penalty))\n",
    "        if (len(loss_history) > 1 and np.abs(loss_history[-1] - loss_history[-2]) < 1e-5):\n",
    "            break\n",
    "    return loss_history, coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "coefficients = np.random.normal(0,0.5,3).reshape(3,1)\n",
    "\n",
    "penalty = 1e-8\n",
    "\n",
    "cost_history, params_optimal = regularized_gradient_descent(R1_TRAIN, TRAIN_LABELS, coefficients, learningRate, penalty)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(range(len(cost_history)), cost_history, 'b')\n",
    "\n",
    "plt.title(\"Convergence Graph of Regularized Loss Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(params_optimal, R1_TEST, TEST_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Implement a 5-fold cross validation procedure to find the optimal $\\lambda$ value for both Representation 1 and 2. Experiment with at least three different $\\lambda$ values between 0 and 1. Report the mean/std of cross validation accuracy of every representation/parameter combination as a table and clearly mark the best configuration in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.21794871794871795\n",
      "0.0\n",
      "0.9968051118210862\n",
      "0.9967948717948718\n",
      "0.9871794871794872\n",
      "0.6762820512820513\n",
      "0.40705128205128205\n",
      "0.9936102236421726\n",
      "0.9935897435897436\n",
      "0.9871794871794872\n",
      "0.8942307692307693\n",
      "0.8397435897435898\n",
      "0.9904153354632588\n",
      "0.9871794871794872\n",
      "0.9871794871794872\n",
      "0.9230769230769231\n",
      "0.8910256410256411\n",
      "0.9904153354632588\n",
      "0.9839743589743589\n",
      "0.9871794871794872\n",
      "0.9326923076923077\n",
      "0.9038461538461539\n",
      "[[1.         1.         1.         0.21794872 0.        ]\n",
      " [0.99680511 0.99679487 0.98717949 0.67628205 0.40705128]\n",
      " [0.99361022 0.99358974 0.98717949 0.89423077 0.83974359]\n",
      " [0.99041534 0.98717949 0.98717949 0.92307692 0.89102564]\n",
      " [0.99041534 0.98397436 0.98717949 0.93269231 0.90384615]]\n"
     ]
    }
   ],
   "source": [
    "K_VALIDATION = 5\n",
    "\n",
    "SPLITTED_TRAIN_1 = np.array_split(R1_TRAIN,K_VALIDATION)\n",
    "SPLITTED_LABEL_1 = np.array_split(TRAIN_LABELS,K_VALIDATION)\n",
    "\n",
    "penalty_parameters = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "learning_rate = 0.1\n",
    "\n",
    "accuracy_matrix = np.zeros((5,5))\n",
    "\n",
    "for itr_1 in range(K_VALIDATION):\n",
    "    current_penalty = penalty_parameters[itr_1]\n",
    "    for itr_2 in range(K_VALIDATION):\n",
    "        coefficients = np.random.normal(0,0.5,3).reshape(3,1)\n",
    "        CROSS_TRAIN_1 = np.empty((0,3))\n",
    "        CROSS_LABEL_1 = np.empty((0,1))\n",
    "        for part in range(K_VALIDATION):\n",
    "            if part != itr_2:\n",
    "                CROSS_TRAIN_1 = np.vstack((CROSS_TRAIN_1, SPLITTED_TRAIN_1[part]))\n",
    "                CROSS_LABEL_1 = np.vstack((CROSS_LABEL_1, SPLITTED_LABEL_1[part]))\n",
    "        _, params_optimal = regularized_gradient_descent(CROSS_TRAIN_1, CROSS_LABEL_1, coefficients, learning_rate, current_penalty)\n",
    "        p = accuracy(params_optimal, SPLITTED_TRAIN_1[itr_2], SPLITTED_LABEL_1[itr_2])\n",
    "        accuracy_matrix[itr_1][itr_2] = p\n",
    "\n",
    "print(accuracy_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluation (25 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Train the logistic regression classifier on Representation 1 and 2 with the best learning rate you decide. Similarly, train the regularized logistic regression classifier with the best $\\lambda$ you obtained by 5-fold cross validation. Report the training and test classification accuracy as \n",
    "\\begin{align*}\n",
    "\\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}}x100\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points)** Visualize the decision boundary (the line that is given by $\\mathbf{w}^{T}x=0$) obtained from the logistic regression classifier learned without regularization. For this purpose, use only Representation 1. Provide two scatter plots for training and test data points with the decision boundary shown on each of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Comment on your work in your report. Include the answers for the following questions in your discussion. \n",
    "\n",
    "* Did regularization improve the generalization performance (did it help reducing the gap between training and test accuracies/errors)? Did you observe any difference between using Representation 1 and 2?\n",
    "* Which feature set did give the best results? Which one is more discriminative?\n",
    "* What would be your next step to improve test accuracy? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
